
##Justice Fraser
	3 Factors justifying optimism about AI
		exponential growth in computer power
		availability of large data sets
		emergence of really powerful algorithms..

##Work on the future legislation on restriction is starting
such as:
	Where does liability lie when AI causes harm?
	is AI a product within the consumer protection regime.
	EU proposed software liability
	EU Discussing AI Act, and regulation of AI
		A court could ask for explanation about how AI was designed.
		Proposes to ban AI to:
			Get rid of High Risk AI
			Social scanning AI
		Seeks to also solve eonomic loss due to infringment of rights.
	
##It is not possible for systems of large size for any one person to predict how the software will work in the future, such as PFI Contracts.
	Such as meta's source code, or amazons 2 bllion lines of code.

Lethal Autonomous Weapons Systems
	Weapons system that once activated can select and eliminate targets without future intervention of a user.
	Think  valorant
	Lol
	Non-combattants are more at risk due to this,
	Humans have the moral agency to choose to kill, even in the batteffield where killing is normalised.
	Do we make the assumption that all human operators would act with their moral agency?
	Should there be a moral standard of ethical behavior in the machines, and should we expect to give it to the machines even if we do not have it in amongst ourselves.

##AI in Healthcare
	Could Remove empathy from the triage, if used to triage patients.
	Apply problem solving techniques to aid with the treatment of patients.
	Usually a triage would use their personal database of their experience, and their knowledge of medical literature to diagnose a traige.
		Surely we could use AI to scan for these.
		Can AI be used to reliantly diagnose, since there is always discrimination in the teaching of machine learning algorithms..


We expect more from machines since they can perform better than humans in certain respects.

Robots don't get stressed on the battlefield, they don't get tired, or scared

People have suffered real harm from Machine Learning.

We have real and harmful human limitations, but we also have very good reasons to respect more of machines than we do of ourselves. 

If we create the data which AI is trained, should we justify the fact that they should be better than us?

There are very good reasons to espect LAWS to perform better than humans in certain ways.

We have good reasons to expect more of autonomous systems.
Is it a mistake to think about them violating human dignity

Should we give the burden of mitigating harms given by generative AI, to the people in wider socierty. Such as teachers with the release of Large Language Models (LLMs)









In England1.2million people are waiting for community mental health support.

We hope that machines can begin to adress the scarcity of human workpower,

Assistive AI can help people stay at home for longer,and thus removin

We should not allow this importance of independence due to machines, to allow for us to keep the human connection in social care.

Are we pushing our expectations of AI too high?



Do greater technological advances actually assist and lead to a better quality of life in society or not?
	How many people  in high power jobs were working at 11pm to check their emails.
	Quality of life has been deteriorated by lack of human connections.
	Technoogically better doesnt always mean higher quality of life
		Look at B737 Max crashes due to software failure.

The AI revolution could possibly give us a great boost forwards,
But are we going to use this to improve what we do or are we goanna try to fill the gaps.












When humans are doing certain jobs, there are certain things that humans can bring to the tables compared to AI.




What role do you think that emotion plays in your role as a judge that an AI couldn't replace?
	Can you program Mercy into a machine?
	We put differnet values on the weight of a dispute, and we need to understand the weight of a dispute before we can decide whether an AI model would be appropriate, such as AI is appropriate for Ebay Seller Disputes, but not for disputes relating to custody of children.




We think AI to be used as an extension of our abilities, but we always show their shortcomings, when we think about implementing them do we really want to remove human decision making at all points? Do we want to give them all these choices? Will AI take over? 
	People under pressure may not know when to use and when not to use AI, such as care workers using LLMs to aid in them giving their care.

How high is the bar needed to be for us to chose AI over human decision-making, such as in automotive control?
	It depends on what the trade-offs are.
		What is the cost?
	We are lousy drivers compared to the machine, we all think we are good as well, which adds to the problem.
	We need to think about the cases where the software fails, since there is noone to blame, if something happens when someone is in an autonomous car, whilst there is with driver error.
	Human's do not think in a utilitarian view.
	[[Unintended Acceleration Syndrome]] - Failure with software in automated automotives.
	[[Do we feel better when retribution is given?]]
	[[grief]] has value
	

What about the goods of performance vs the goods of failure?
	The trade-offs need not have a single answers,, since there is cultural meaning investing in certain things. Its not a subjective issue, but it is a contextual issue since some cultures value things more than others.

PFI contracts aren't overly complex, the agreements are complex.
	Simple AI systems are alot easier than more complicated AI ones.

When is it important that a hole be made by a human, vs to be made by a drill even if the hole isn't [[ass]] good?
	It depends to who coded the machine, who trained it, because [[subconcious bias]] is profound in the design of a machine.
	Do we value things instrumentally or non-instrumentally.

Might some moral sentiments be important for judgements if we think about judgements beyond a solely legal case?
	Legal decisions and [[judgements]] cannot be equated to legal cases
	There could be an argument for NO-FAULT Litigation for the failure for AI.
	English Law presumes that a computer system works correctly.
	We are now at a time to decide what are the experiences and values that we do not want to give up.
	It boils down to  [[Human Rights]] , and wether we want to hold companies accountable.